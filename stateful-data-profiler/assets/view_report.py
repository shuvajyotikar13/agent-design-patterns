#!/usr/bin/env python3
# =============================================================================
# COMPONENT: stateful-data-profiler (Report Viewer)
# -----------------------------------------------------------------------------
# Copyright 2026 Shuva Jyoti Kar
# License: Apache 2.0
# =============================================================================
"""
Memory Indexer & Context Compressor
===================================

Overview:
---------
In Agentic Systems, "Memory" can quickly become a liability if it grows too large. 
If an Agent has profiled 50 files, reading the raw JSON cache to remember what 
it knows would exhaust the token context window and trigger a Denial of Wallet.

This script implements the "Context Compressor" pattern. It acts as the Agent's 
"Librarian," reading the dense cognitive cache and projecting it into a highly 
token-efficient, Markdown-formatted index. This allows the LLM to scan its 
memory footprint at a glance before deciding which file requires deeper analysis.
"""

import json
import os  
import datetime

# The local memory store generated by the profiling tool
CACHE_FILE = ".antigravity_data_cache.json"

def render_report():
    """
    Reads the local agentic cache and generates a token-optimized Markdown table.
    
    Markdown is used intentionally: LLMs are natively trained on Markdown 
    and parse tabular data in this format with high semantic accuracy, 
    making it the most efficient way to inject structured state into the prompt.
    """
    
    # =========================================================================
    # 1. Guardrails & State Validation
    # =========================================================================
    # If the memory file doesn't exist, we provide actionable feedback so the 
    # LLM knows it must run the analyzer tool first, preventing a dead-end turn.
    if not os.path.exists(CACHE_FILE):
        print("No knowledge base found. Run the analyzer first.")
        return

    try:
        with open(CACHE_FILE, 'r') as f:
            cache = json.load(f)
    except json.JSONDecodeError:
        # Handle corruption gracefully without throwing a raw Python stack trace 
        # that might confuse the LLM.
        print("Error: Cache file is corrupt.")
        return

    # =========================================================================
    # 2. The Presentation Layer (Token-Optimized Output)
    # =========================================================================
    print("\n# ðŸ“š The Librarian's Report")
    
    # Injecting the timestamp gives the LLM temporal awareness of its state 
    # (e.g., "Is this cache from today or yesterday?")
    print(f"*Generated at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    # Table Header
    print(f"| {'Filename':<30} | {'Status':<10} | {'Summary':<50} |")
    print(f"|{'-'*32}|{'-'*12}|{'-'*52}|")

    file_count = 0
    
    # =========================================================================
    # 3. Data Projection & Truncation Loop
    # =========================================================================
    for filepath, data in cache.items():
        filename = os.path.basename(filepath)
        
        # Truncation is a deliberate token-budgeting strategy. 
        # We enforce strict column widths to prevent long strings from breaking 
        # the Markdown table rendering in the LLM's context window.
        if len(filename) > 28:
            filename = filename[:25] + "..."
            
        summary = data.get('summary', 'No summary available.')
        
        # Keep the summary dense and actionable. If the LLM needs the full 
        # deep-dive stats, it knows it can invoke the profiler tool on that file.
        if len(summary) > 48:
            summary = summary[:45] + "..."

        print(f"| {filename:<30} | {'Cached':<10} | {summary:<50} |")
        file_count += 1

    # =========================================================================
    # 4. Context Footer
    # =========================================================================
    # Summarizes the scope of the agent's knowledge explicitly.
    print(f"\n**Total Files Tracked:** {file_count}")

if __name__ == "__main__":
    # No CLI arguments needed for this specific viewer; it acts strictly 
    # on the predefined environment cache file.
    render_report()
