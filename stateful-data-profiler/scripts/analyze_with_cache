#!/usr/bin/env python3
# =============================================================================
# COMPONENT: stateful-data-profiler
# -----------------------------------------------------------------------------
# Copyright 2026 Shuva Jyoti Kar
# License: Apache 2.0
# =============================================================================
import sys
import os
import json
import hashlib
import argparse

# Dependency Check: The "Staff Engineer" way to handle missing tools
try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    print("WARNING: pandas not found. Advanced CSV profiling disabled.")

CACHE_FILE = ".antigravity_data_cache.json"

def get_file_hash(filepath):
    """Generates MD5 hash to detect file changes."""
    hasher = hashlib.md5()
    try:
        with open(filepath, 'rb') as f:
            # Read in chunks to be memory safe for large files
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        return hasher.hexdigest()
    except FileNotFoundError:
        print(f"ERROR: File {filepath} not found.")
        sys.exit(1)

def profile_csv(filepath):
    """
    Performs the deep cognitive analysis on CSV files.
    Returns: (stats_dict, summary_string)
    """
    if not HAS_PANDAS:
        return {}, "Pandas missing. Generic analysis only."

    try:
        df = pd.read_csv(filepath)
        rows, cols = df.shape
        
        # Calculate unique elements per column (Cardinality)
        # This helps the agent guess ID columns vs Categorical columns
        unique_counts = df.nunique().to_dict()
        
        # Identify potential ID columns (where unique count == row count)
        potential_ids = [col for col, count in unique_counts.items() if count == rows]
        
        stats = {
            "type": "csv",
            "rows": rows,
            "columns": cols,
            "column_names": list(df.columns),
            "unique_values": unique_counts,
            "potential_keys": potential_ids
        }

        # Create a dense, information-rich summary for the 'view_report' script
        summary = f"CSV: {rows:,} rows, {cols} cols. Key Candidates: {potential_ids if potential_ids else 'None'}"
        return stats, summary

    except Exception as e:
        return {}, f"Error analyzing CSV structure: {str(e)}"

def analyze_data(filepath):
    # 1. Cognitive Step: Check Memory
    current_hash = get_file_hash(filepath)
    cache = {}
    
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r') as f:
                cache = json.load(f)
        except json.JSONDecodeError:
            cache = {} 
            
    # 2. Decision: Recall vs. Re-read
    # We check if the file is in cache, the hash matches, AND if we have the new 'stats' field.
    # If the cache is from the old version (no stats), we force a re-analysis.
    is_cached = filepath in cache and cache[filepath]['hash'] == current_hash
    has_stats = 'stats' in cache.get(filepath, {})
    
    if is_cached and (has_stats or not filepath.endswith('.csv')):
        print(f"MEMORY RECALL: I have already analyzed this file.")
        print(f"SUMMARY: {cache[filepath]['summary']}")
        
        # If we have detailed stats, print them so the LLM sees them in this turn
        if 'stats' in cache[filepath]:
            print(f"\n[DETAILED PROFILE RECALLED]")
            print(json.dumps(cache[filepath]['stats'], indent=2))
        return

    # 3. Action: Perform Analysis
    print("ANALYZING: Reading file and calculating deep stats...")
    
    file_stats = {}
    summary = ""
    file_size = os.path.getsize(filepath)

    if filepath.endswith('.csv'):
        file_stats, summary = profile_csv(filepath)
    else:
        summary = f"File {os.path.basename(filepath)} is {file_size} bytes (Generic)."

    # 4. Cognitive Step: Commit to Memory
    cache[filepath] = {
        'hash': current_hash,
        'summary': summary,
        'stats': file_stats, # Storing the structured knowledge
        'analyzed_at': str(os.path.getmtime(filepath))
    }
    
    with open(CACHE_FILE, 'w') as f:
        json.dump(cache, f, indent=2)
        
    print(f"ANALYSIS COMPLETE: {summary}")
    if file_stats:
        print(json.dumps(file_stats, indent=2))

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--filepath", required=True)
    args = parser.parse_args()
    
    analyze_data(args.filepath)
